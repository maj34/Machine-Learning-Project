{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#CC3D3D\"><p>\n",
    "# Making Word2Vec Features\n",
    "    \n",
    "<font color=\"black\"><p>\n",
    "- [W2V Feature](#W2V-Feature)\n",
    "    - [corner_nm](#corner_nm)\n",
    "    - [brd_nm](#brd_nm)\n",
    "    - [pc_nm](#pc_nm)\n",
    "    - [part_nm](#part_nm)\n",
    "    - [customer_info](#customer_info)\n",
    "- [Feature Merge](#Feature-Merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/X_train.csv', encoding = 'cp949')\n",
    "test = pd.read_csv('../input/X_test.csv', encoding = 'cp949')\n",
    "y_train = pd.read_csv('../input/y_train.csv', encoding = 'cp949')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## corner_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word2vec_corner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word2vec_corner.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "### Read data\n",
    "train = pd.read_csv('../input/X_train.csv', encoding = 'cp949')\n",
    "test = pd.read_csv('../input/X_test.csv', encoding = 'cp949')\n",
    "\n",
    "\n",
    "### Make corpus\n",
    "p_level = 'corner_nm'  # 상품 분류 수준\n",
    "\n",
    "# W2V 학습데이터가 부족하여 구매한 상품 목록으로부터 n배 oversampling을 수행\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=True))  # 복원추출\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('custid')[p_level].agg(oversample, 20))\n",
    "test_corpus = list(test.groupby('custid')[p_level].agg(oversample, 20))\n",
    "\n",
    "\n",
    "### Training the Word2Vec model\n",
    "num_features = 100 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 5 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        vector_size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "# 필요없는 메모리 unload\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                #np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "# W2V 기반 feature 생성\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "# 학습용과 제출용 데이터로 분리\n",
    "X_train_corner = pd.concat([pd.DataFrame({'custid': np.sort(train['custid'].unique())}), train_features], axis=1)#.to_csv('X_train_buyer.csv', index=False)\n",
    "X_test_corner = pd.concat([pd.DataFrame({'custid': np.sort(test['custid'].unique())}), test_features], axis=1)#.to_csv('X_test_buyer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run word2vec_corner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## brd_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile word2vec_brd.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "### Read data\n",
    "train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "\n",
    "\n",
    "### Make corpus\n",
    "p_level = 'brd_nm'  # 상품 분류 수준\n",
    "\n",
    "# W2V 학습데이터가 부족하여 구매한 상품 목록으로부터 n배 oversampling을 수행\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=True))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('custid')[p_level].agg(oversample, 20))\n",
    "test_corpus = list(test.groupby('custid')[p_level].agg(oversample, 20))\n",
    "\n",
    "\n",
    "### Training the Word2Vec model\n",
    "num_features = 300 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 5 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        vector_size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "# 필요없는 메모리 unload\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                #np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "# W2V 기반 feature 생성\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "# 학습용과 제출용 데이터로 분리\n",
    "X_train_brd = pd.concat([pd.DataFrame({'custid': np.sort(train['custid'].unique())}), train_features], axis=1)#.to_csv('X_train_buyer.csv', index=False)\n",
    "X_test_brd = pd.concat([pd.DataFrame({'custid': np.sort(test['custid'].unique())}), test_features], axis=1)#.to_csv('X_test_buyer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Administrator\\Desktop\\3학년\\머신러닝\\Competition\\word2vec_brd.py:46: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "%run word2vec_brd.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pc_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word2vec_pc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word2vec_pc.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "### Read data\n",
    "train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "\n",
    "\n",
    "### Make corpus\n",
    "p_level = 'pc_nm'  # 상품 분류 수준\n",
    "\n",
    "# W2V 학습데이터가 부족하여 구매한 상품 목록으로부터 n배 oversampling을 수행\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=True))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('custid')[p_level].agg(oversample, 20))\n",
    "test_corpus = list(test.groupby('custid')[p_level].agg(oversample, 20))\n",
    "\n",
    "\n",
    "### Training the Word2Vec model\n",
    "num_features = 50 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 5 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        vector_size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "# 필요없는 메모리 unload\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                #np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "# W2V 기반 feature 생성\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "# 학습용과 제출용 데이터로 분리\n",
    "X_train_pc = pd.concat([pd.DataFrame({'custid': np.sort(train['custid'].unique())}), train_features], axis=1)#.to_csv('X_train_buyer.csv', index=False)\n",
    "X_test_pc = pd.concat([pd.DataFrame({'custid': np.sort(test['custid'].unique())}), test_features], axis=1)#.to_csv('X_test_buyer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Desktop\\3학년\\머신러닝\\Competition\\word2vec_pc.py:46: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "%run word2vec_pc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## part_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word2vec_part.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word2vec_part.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "### Read data\n",
    "train = pd.read_csv('X_train.csv', encoding='cp949')\n",
    "test = pd.read_csv('X_test.csv', encoding='cp949')\n",
    "\n",
    "\n",
    "### Make corpus\n",
    "p_level = 'part_nm'  # 상품 분류 수준\n",
    "\n",
    "# W2V 학습데이터가 부족하여 구매한 상품 목록으로부터 n배 oversampling을 수행\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=True))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('custid')[p_level].agg(oversample, 20))\n",
    "test_corpus = list(test.groupby('custid')[p_level].agg(oversample, 20))\n",
    "\n",
    "\n",
    "### Training the Word2Vec model\n",
    "num_features = 100 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 5 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        vector_size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "# 필요없는 메모리 unload\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                #np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "# W2V 기반 feature 생성\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "# 학습용과 제출용 데이터로 분리\n",
    "X_train_part = pd.concat([pd.DataFrame({'custid': np.sort(train['custid'].unique())}), train_features], axis=1)#.to_csv('X_train_buyer.csv', index=False)\n",
    "X_test_part = pd.concat([pd.DataFrame({'custid': np.sort(test['custid'].unique())}), test_features], axis=1)#.to_csv('X_test_buyer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Desktop\\3학년\\머신러닝\\Competition\\word2vec_part.py:46: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "%run word2vec_part.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## customer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting word2vec_customer_info.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile word2vec_customer_info.py\n",
    "\n",
    "### Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "### Read data\n",
    "train = pd.read_csv('../input/X_train.csv', encoding='cp949')\n",
    "test = pd.read_csv('../input/X_test.csv', encoding='cp949')\n",
    "train['customer_info'] = train['brd_nm'].astype(str) + '_' + train['corner_nm'].astype(str) + '_' + train['pc_nm'].astype(str) + '_' + train['part_nm'].astype(str) + '_' + train['str_nm'].astype(str) + '_' + train['team_nm'].astype(str) + '_' + train['buyer_nm'].astype(str)\n",
    "test['customer_info'] = test['brd_nm'].astype(str) + '_' + test['corner_nm'].astype(str) + '_' + test['pc_nm'].astype(str) + '_' + test['part_nm'].astype(str) + '_' + test['str_nm'].astype(str) + '_' + test['team_nm'].astype(str) + '_' + test['buyer_nm'].astype(str)\n",
    "\n",
    "### Make corpus\n",
    "p_level = 'customer_info'  # 상품 분류 수준\n",
    "\n",
    "# W2V 학습데이터가 부족하여 구매한 상품 목록으로부터 n배 oversampling을 수행\n",
    "def oversample(x, n, seed=0):\n",
    "    if n == 0:\n",
    "        return list(x)\n",
    "    uw = np.unique(x)\n",
    "    bs = np.array([])\n",
    "    np.random.seed(seed)\n",
    "    for j in range(n):\n",
    "        bs = np.append(bs, np.random.choice(uw, len(uw), replace=True))\n",
    "    return list(bs)\n",
    "\n",
    "train_corpus = list(train.groupby('custid')[p_level].agg(oversample, 20))\n",
    "test_corpus = list(test.groupby('custid')[p_level].agg(oversample, 20))\n",
    "\n",
    "\n",
    "### Training the Word2Vec model\n",
    "num_features = 100 # 단어 벡터 차원 수\n",
    "min_word_count = 1 # 최소 단어 수\n",
    "context = 5 # 학습 윈도우(인접한 단어 리스트) 크기\n",
    "\n",
    "# 초기화 및 모델 학습\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# 모델 학습\n",
    "w2v = word2vec.Word2Vec(train_corpus, \n",
    "                        vector_size=num_features, \n",
    "                        min_count=min_word_count,\n",
    "                        window=context,\n",
    "                        seed=0, workers=1)\n",
    "# 필요없는 메모리 unload\n",
    "w2v.init_sims(replace=True)\n",
    "\n",
    "\n",
    "### Make features\n",
    "# 구매상품에 해당하는 벡터의 평균/최소/최대 벡터를 feature로 만드는 전처리기\n",
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = num_features\n",
    "    def fit(self, X):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.hstack([\n",
    "                np.max([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.min([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0),                \n",
    "                #np.std([self.word2vec[w] for w in words if w in self.word2vec] or [np.zeros(self.dim)], axis=0)                \n",
    "            ]) \n",
    "            for words in X\n",
    "        ]) \n",
    "\n",
    "# W2V 기반 feature 생성\n",
    "train_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).fit(train_corpus).transform(train_corpus))\n",
    "test_features = pd.DataFrame(EmbeddingVectorizer(w2v.wv).transform(test_corpus))\n",
    "\n",
    "train_features.columns = ['v'+f'{c+1:03d}' for c in train_features.columns]\n",
    "test_features.columns = ['v'+f'{c+1:03d}' for c in test_features.columns]\n",
    "\n",
    "# 학습용과 제출용 데이터로 분리\n",
    "X_train_customer_info = pd.concat([pd.DataFrame({'custid': np.sort(train['custid'].unique())}), train_features], axis=1)#.to_csv('X_train_buyer.csv', index=False)\n",
    "X_test_customer_info = pd.concat([pd.DataFrame({'custid': np.sort(test['custid'].unique())}), test_features], axis=1)#.to_csv('X_test_buyer.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Administrator\\Desktop\\3학년\\머신러닝\\Competition\\Round2\\notebooks\\word2vec_customer_info.py:47: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v.init_sims(replace=True)\n"
     ]
    }
   ],
   "source": [
    "%run word2vec_customer_info.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_corner['custid']\n",
    "del X_test_corner['custid']\n",
    "del X_train_brd['custid']\n",
    "del X_test_brd['custid']\n",
    "del X_train_pc['custid']\n",
    "del X_test_pc['custid']\n",
    "del X_train_part['custid']\n",
    "del X_test_part['custid']\n",
    "del X_train_customer_info['custid']\n",
    "del X_test_customer_info['custid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_corner.columns = X_train_corner.columns.map(lambda x : \"corner_\" + str(x))\n",
    "X_test_corner.columns = X_test_corner.columns.map(lambda x : \"corner_\" + str(x))\n",
    "X_train_brd.columns = X_train_brd.columns.map(lambda x : \"brd_\" + str(x))\n",
    "X_test_brd.columns = X_test_brd.columns.map(lambda x : \"brd_\" + str(x))\n",
    "X_train_pc.columns = X_train_pc.columns.map(lambda x : \"pc_\" + str(x))\n",
    "X_test_pc.columns = X_test_pc.columns.map(lambda x : \"pc_\" + str(x))\n",
    "X_train_part.columns = X_train_part.columns.map(lambda x : \"part_\" + str(x))\n",
    "X_test_part.columns = X_test_part.columns.map(lambda x : \"part_\" + str(x))\n",
    "X_train_customer_info.columns = X_train_customer_info.columns.map(lambda x : \"customer_info_\" + str(x))\n",
    "X_test_customer_info.columns = X_test_customer_info.columns.map(lambda x : \"customer_info_\" + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_info_v001</th>\n",
       "      <th>customer_info_v002</th>\n",
       "      <th>customer_info_v003</th>\n",
       "      <th>customer_info_v004</th>\n",
       "      <th>customer_info_v005</th>\n",
       "      <th>customer_info_v006</th>\n",
       "      <th>customer_info_v007</th>\n",
       "      <th>customer_info_v008</th>\n",
       "      <th>customer_info_v009</th>\n",
       "      <th>customer_info_v010</th>\n",
       "      <th>...</th>\n",
       "      <th>part_v291</th>\n",
       "      <th>part_v292</th>\n",
       "      <th>part_v293</th>\n",
       "      <th>part_v294</th>\n",
       "      <th>part_v295</th>\n",
       "      <th>part_v296</th>\n",
       "      <th>part_v297</th>\n",
       "      <th>part_v298</th>\n",
       "      <th>part_v299</th>\n",
       "      <th>part_v300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.126028</td>\n",
       "      <td>0.238448</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.129367</td>\n",
       "      <td>0.157828</td>\n",
       "      <td>0.190237</td>\n",
       "      <td>0.161458</td>\n",
       "      <td>0.315062</td>\n",
       "      <td>0.192215</td>\n",
       "      <td>0.175752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.071623</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>-0.078153</td>\n",
       "      <td>0.055440</td>\n",
       "      <td>0.012121</td>\n",
       "      <td>-0.097974</td>\n",
       "      <td>0.023984</td>\n",
       "      <td>0.015453</td>\n",
       "      <td>0.082945</td>\n",
       "      <td>-0.009952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.126246</td>\n",
       "      <td>0.181655</td>\n",
       "      <td>0.175741</td>\n",
       "      <td>0.184657</td>\n",
       "      <td>0.206774</td>\n",
       "      <td>0.242676</td>\n",
       "      <td>0.233091</td>\n",
       "      <td>0.166628</td>\n",
       "      <td>0.209461</td>\n",
       "      <td>0.213504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004586</td>\n",
       "      <td>0.019767</td>\n",
       "      <td>-0.033093</td>\n",
       "      <td>-0.003008</td>\n",
       "      <td>-0.001773</td>\n",
       "      <td>-0.038660</td>\n",
       "      <td>-0.021493</td>\n",
       "      <td>-0.024507</td>\n",
       "      <td>0.052645</td>\n",
       "      <td>-0.018186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.160387</td>\n",
       "      <td>0.144859</td>\n",
       "      <td>0.138732</td>\n",
       "      <td>0.222700</td>\n",
       "      <td>0.194777</td>\n",
       "      <td>0.205199</td>\n",
       "      <td>0.216914</td>\n",
       "      <td>0.207435</td>\n",
       "      <td>0.208292</td>\n",
       "      <td>0.120357</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011320</td>\n",
       "      <td>0.057334</td>\n",
       "      <td>0.077167</td>\n",
       "      <td>-0.093443</td>\n",
       "      <td>-0.093308</td>\n",
       "      <td>0.119095</td>\n",
       "      <td>-0.027661</td>\n",
       "      <td>-0.012306</td>\n",
       "      <td>-0.116368</td>\n",
       "      <td>0.025222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.124489</td>\n",
       "      <td>-0.037535</td>\n",
       "      <td>0.138732</td>\n",
       "      <td>0.026085</td>\n",
       "      <td>0.159970</td>\n",
       "      <td>0.104852</td>\n",
       "      <td>0.077204</td>\n",
       "      <td>0.167743</td>\n",
       "      <td>0.084491</td>\n",
       "      <td>-0.111642</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075549</td>\n",
       "      <td>0.027678</td>\n",
       "      <td>0.120764</td>\n",
       "      <td>-0.060207</td>\n",
       "      <td>-0.114727</td>\n",
       "      <td>0.126929</td>\n",
       "      <td>0.009970</td>\n",
       "      <td>-0.117949</td>\n",
       "      <td>-0.029883</td>\n",
       "      <td>0.004025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.139541</td>\n",
       "      <td>0.168830</td>\n",
       "      <td>0.089678</td>\n",
       "      <td>0.086368</td>\n",
       "      <td>0.132142</td>\n",
       "      <td>0.190312</td>\n",
       "      <td>0.115949</td>\n",
       "      <td>0.142076</td>\n",
       "      <td>0.049963</td>\n",
       "      <td>-0.007831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035434</td>\n",
       "      <td>0.023045</td>\n",
       "      <td>0.039544</td>\n",
       "      <td>-0.052808</td>\n",
       "      <td>-0.066477</td>\n",
       "      <td>0.063561</td>\n",
       "      <td>-0.044272</td>\n",
       "      <td>-0.090704</td>\n",
       "      <td>0.004998</td>\n",
       "      <td>0.038559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14375</th>\n",
       "      <td>0.079857</td>\n",
       "      <td>-0.012189</td>\n",
       "      <td>0.127850</td>\n",
       "      <td>0.017006</td>\n",
       "      <td>0.178856</td>\n",
       "      <td>0.067744</td>\n",
       "      <td>0.059685</td>\n",
       "      <td>0.055509</td>\n",
       "      <td>0.038627</td>\n",
       "      <td>0.077512</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141780</td>\n",
       "      <td>0.031884</td>\n",
       "      <td>-0.017108</td>\n",
       "      <td>0.079073</td>\n",
       "      <td>-0.035915</td>\n",
       "      <td>-0.089745</td>\n",
       "      <td>0.069129</td>\n",
       "      <td>0.030556</td>\n",
       "      <td>0.029815</td>\n",
       "      <td>-0.003495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14376</th>\n",
       "      <td>-0.122372</td>\n",
       "      <td>0.168830</td>\n",
       "      <td>0.048312</td>\n",
       "      <td>0.018549</td>\n",
       "      <td>0.116079</td>\n",
       "      <td>0.020438</td>\n",
       "      <td>-0.067901</td>\n",
       "      <td>-0.003937</td>\n",
       "      <td>-0.015879</td>\n",
       "      <td>-0.007831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107623</td>\n",
       "      <td>-0.003805</td>\n",
       "      <td>-0.021734</td>\n",
       "      <td>-0.011623</td>\n",
       "      <td>0.058924</td>\n",
       "      <td>-0.024888</td>\n",
       "      <td>-0.085418</td>\n",
       "      <td>-0.138603</td>\n",
       "      <td>0.177582</td>\n",
       "      <td>0.056818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14377</th>\n",
       "      <td>0.074228</td>\n",
       "      <td>0.082197</td>\n",
       "      <td>-0.003594</td>\n",
       "      <td>-0.051917</td>\n",
       "      <td>0.180146</td>\n",
       "      <td>0.037355</td>\n",
       "      <td>0.149413</td>\n",
       "      <td>0.159084</td>\n",
       "      <td>0.075022</td>\n",
       "      <td>0.121613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141780</td>\n",
       "      <td>0.031884</td>\n",
       "      <td>-0.017108</td>\n",
       "      <td>0.079073</td>\n",
       "      <td>-0.035915</td>\n",
       "      <td>-0.089745</td>\n",
       "      <td>0.069129</td>\n",
       "      <td>0.030556</td>\n",
       "      <td>0.029815</td>\n",
       "      <td>-0.003495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14378</th>\n",
       "      <td>0.088175</td>\n",
       "      <td>0.194241</td>\n",
       "      <td>0.149394</td>\n",
       "      <td>0.138779</td>\n",
       "      <td>0.169383</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.168674</td>\n",
       "      <td>0.119944</td>\n",
       "      <td>0.170581</td>\n",
       "      <td>0.034228</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013600</td>\n",
       "      <td>0.057094</td>\n",
       "      <td>0.059845</td>\n",
       "      <td>-0.119288</td>\n",
       "      <td>-0.063213</td>\n",
       "      <td>0.107178</td>\n",
       "      <td>-0.040400</td>\n",
       "      <td>-0.058523</td>\n",
       "      <td>-0.092707</td>\n",
       "      <td>0.006307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14379</th>\n",
       "      <td>0.024933</td>\n",
       "      <td>0.168830</td>\n",
       "      <td>0.065444</td>\n",
       "      <td>0.068446</td>\n",
       "      <td>0.116079</td>\n",
       "      <td>0.163152</td>\n",
       "      <td>0.076098</td>\n",
       "      <td>0.114064</td>\n",
       "      <td>0.042758</td>\n",
       "      <td>0.082845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040672</td>\n",
       "      <td>0.030554</td>\n",
       "      <td>-0.037014</td>\n",
       "      <td>0.025457</td>\n",
       "      <td>-0.007433</td>\n",
       "      <td>-0.059898</td>\n",
       "      <td>-0.029628</td>\n",
       "      <td>-0.030282</td>\n",
       "      <td>0.089808</td>\n",
       "      <td>0.020351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14380 rows × 1950 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       customer_info_v001  customer_info_v002  customer_info_v003  \\\n",
       "0                0.126028            0.238448            0.010795   \n",
       "1                0.126246            0.181655            0.175741   \n",
       "2                0.160387            0.144859            0.138732   \n",
       "3                0.124489           -0.037535            0.138732   \n",
       "4                0.139541            0.168830            0.089678   \n",
       "...                   ...                 ...                 ...   \n",
       "14375            0.079857           -0.012189            0.127850   \n",
       "14376           -0.122372            0.168830            0.048312   \n",
       "14377            0.074228            0.082197           -0.003594   \n",
       "14378            0.088175            0.194241            0.149394   \n",
       "14379            0.024933            0.168830            0.065444   \n",
       "\n",
       "       customer_info_v004  customer_info_v005  customer_info_v006  \\\n",
       "0                0.129367            0.157828            0.190237   \n",
       "1                0.184657            0.206774            0.242676   \n",
       "2                0.222700            0.194777            0.205199   \n",
       "3                0.026085            0.159970            0.104852   \n",
       "4                0.086368            0.132142            0.190312   \n",
       "...                   ...                 ...                 ...   \n",
       "14375            0.017006            0.178856            0.067744   \n",
       "14376            0.018549            0.116079            0.020438   \n",
       "14377           -0.051917            0.180146            0.037355   \n",
       "14378            0.138779            0.169383            0.037109   \n",
       "14379            0.068446            0.116079            0.163152   \n",
       "\n",
       "       customer_info_v007  customer_info_v008  customer_info_v009  \\\n",
       "0                0.161458            0.315062            0.192215   \n",
       "1                0.233091            0.166628            0.209461   \n",
       "2                0.216914            0.207435            0.208292   \n",
       "3                0.077204            0.167743            0.084491   \n",
       "4                0.115949            0.142076            0.049963   \n",
       "...                   ...                 ...                 ...   \n",
       "14375            0.059685            0.055509            0.038627   \n",
       "14376           -0.067901           -0.003937           -0.015879   \n",
       "14377            0.149413            0.159084            0.075022   \n",
       "14378            0.168674            0.119944            0.170581   \n",
       "14379            0.076098            0.114064            0.042758   \n",
       "\n",
       "       customer_info_v010  ...  part_v291  part_v292  part_v293  part_v294  \\\n",
       "0                0.175752  ...   0.071623   0.001848  -0.078153   0.055440   \n",
       "1                0.213504  ...   0.004586   0.019767  -0.033093  -0.003008   \n",
       "2                0.120357  ...  -0.011320   0.057334   0.077167  -0.093443   \n",
       "3               -0.111642  ...  -0.075549   0.027678   0.120764  -0.060207   \n",
       "4               -0.007831  ...  -0.035434   0.023045   0.039544  -0.052808   \n",
       "...                   ...  ...        ...        ...        ...        ...   \n",
       "14375            0.077512  ...   0.141780   0.031884  -0.017108   0.079073   \n",
       "14376           -0.007831  ...  -0.107623  -0.003805  -0.021734  -0.011623   \n",
       "14377            0.121613  ...   0.141780   0.031884  -0.017108   0.079073   \n",
       "14378            0.034228  ...  -0.013600   0.057094   0.059845  -0.119288   \n",
       "14379            0.082845  ...   0.040672   0.030554  -0.037014   0.025457   \n",
       "\n",
       "       part_v295  part_v296  part_v297  part_v298  part_v299  part_v300  \n",
       "0       0.012121  -0.097974   0.023984   0.015453   0.082945  -0.009952  \n",
       "1      -0.001773  -0.038660  -0.021493  -0.024507   0.052645  -0.018186  \n",
       "2      -0.093308   0.119095  -0.027661  -0.012306  -0.116368   0.025222  \n",
       "3      -0.114727   0.126929   0.009970  -0.117949  -0.029883   0.004025  \n",
       "4      -0.066477   0.063561  -0.044272  -0.090704   0.004998   0.038559  \n",
       "...          ...        ...        ...        ...        ...        ...  \n",
       "14375  -0.035915  -0.089745   0.069129   0.030556   0.029815  -0.003495  \n",
       "14376   0.058924  -0.024888  -0.085418  -0.138603   0.177582   0.056818  \n",
       "14377  -0.035915  -0.089745   0.069129   0.030556   0.029815  -0.003495  \n",
       "14378  -0.063213   0.107178  -0.040400  -0.058523  -0.092707   0.006307  \n",
       "14379  -0.007433  -0.059898  -0.029628  -0.030282   0.089808   0.020351  \n",
       "\n",
       "[14380 rows x 1950 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_features_train = pd.concat([X_train_corner, X_train_brd, X_train_pc, X_train_part, X_train_buyer], axis=1) ; X_train_w2v\n",
    "w2v_features_test = pd.concat([X_test_corner, X_test_brd, X_test_pc, X_test_part, X_test_buyer], axis=1) ; X_test_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_features_train.to_csv('w2v_features_train.csv', index=False)\n",
    "w2v_features_test.to_csv('w2v_features_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"#CC3D3D\"><p>\n",
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
